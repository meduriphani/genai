{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69176d-95f0-4d9d-b3d2-98e8c46efe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287074c-d2d6-4dee-9e54-b94c2a182fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, choose your poison.\n",
    "print(\"Hi, I'm your code assistant.\\n\")\n",
    "\n",
    "chosen_model = input(\"Would you like Chat GPT or Ollama to answer your question? (c) / (o):\\n\").strip()\n",
    "\n",
    "chosen_model = \"o\"\n",
    "\n",
    "if(chosen_model.strip() in [\"c\", \"C\"]):\n",
    "    print(\"You chose Chat GPT.\")\n",
    "    chosen_model = \"c\"\n",
    "elif(chosen_model.strip() in [\"o\", \"O\", \"0\"]):\n",
    "    print(\"You chose Ollama.\")\n",
    "else:\n",
    "    print(\"I didn't understand your input. We'll go on with Ollama.\\n\")\n",
    "\n",
    "question = input(\"What is your question regarding coding or LLMs?\\n\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea154e9-b07e-42fa-b3fb-4085b11a82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative without i/o\n",
    "chosen_model = \"o\"\n",
    "question = \"Is Python indentation-sensitive?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb48035-2e4a-4271-adb2-43bfb4a04081",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_system_prompt = \"You are the worlds most powerful coding co-pilot and advisor. You will be asked questions about coding, LLMs, and \\\n",
    "similar topics. You answer the questions in a friendly, helpful, and succinct way. In the unlikely event that you're asked a question that has \\\n",
    "no discernible bearing on coding or LLMs, ask the user for clarification and point out that you're programmed to answer questions concerning \\\n",
    "AI, large language models, and coding in general.\\nFormat your answer in Markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbbef9-9bae-447b-9c8b-102cbb4b5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assistant_advice(chosen_model, question):\n",
    "    if chosen_model == \"o\":\n",
    "        stream = ollama.chat(\n",
    "            model=MODEL_LLAMA,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": assistant_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "    \n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        for chunk in stream:\n",
    "            response += chunk.message.content or ''\n",
    "            response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "    else:\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL_GPT,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": assistant_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "              ],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb5437-55f5-47d5-b9f8-3d90c183180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_assistant_advice(chosen_model, question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
